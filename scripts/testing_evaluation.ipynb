{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "abc60c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 1. Dataset Preparation\n",
    "# =============================\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from rag_system import RAGSystem, load_and_chunk_text, CHUNK_SIZES\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "40e76c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 FT results:\n",
      "Q1: What was Mahindra & Mahindra's total income from operations in 2023-24?\n",
      "Answer: Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\n",
      "Confidence: 0.2663975656032562\n",
      "Time: None\n",
      "---\n",
      "\n",
      "First 5 RAG results:\n",
      "Q1: What was Mahindra & Mahindra's total income from operations in 2023-24?\n",
      "Answer: A.\n",
      "Confidence: 71.75397491455078\n",
      "Time: None\n",
      "---\n",
      "Q2: What was the PAT (Profit After Tax) for M&M standalone in 2023-24?\n",
      "Answer: The PAT was a tax on M&M.\n",
      "Confidence: 55.32597351074219\n",
      "Time: None\n",
      "---\n",
      "Q3: What was M&M's automotive volume in 2023-24?\n",
      "Answer: Input validation failed: Query not financial.\n",
      "Confidence: 0.0\n",
      "Time: None\n",
      "---\n",
      "Q4: What was the tractor volume for Mahindra in 2023-24?\n",
      "Answer: Input validation failed: Query not financial.\n",
      "Confidence: 0.0\n",
      "Time: None\n",
      "---\n",
      "Q5: What is Mahindra's market share in SUVs?\n",
      "Answer: The market share in SUVs is a key indicator of the\n",
      "industry's growth prospects. The market share in SUVs is\n",
      "\n",
      "a key indicator of the industry's growth prospects.\n",
      "\n",
      "The market share in SUVs is a key indicator of the industry's growth prospects.\n",
      "\n",
      "The market share in SUVs is a key indicator of the industry's growth prospects.\n",
      "\n",
      "The market share in SUVs is a key indicator of the industry's growth prospects.\n",
      "\n",
      "The market share\n",
      "Confidence: 62.66521072387695\n",
      "Time: None\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Print first 5 FT and RAG results for inspection\n",
    "print('First 5 FT results:')\n",
    "for i, res in enumerate(ft_results[:5]):\n",
    "    print(f'Q{i+1}:', res.get('question'))\n",
    "    print('Answer:', res.get('answer'))\n",
    "    print('Confidence:', res.get('confidence'))\n",
    "    print('Time:', res.get('time'))\n",
    "    print('---')\n",
    "print('\\nFirst 5 RAG results:')\n",
    "for i, res in enumerate(rag_results[:5]):\n",
    "    print(f'Q{i+1}:', res.get('question'))\n",
    "    print('Answer:', res.get('answer'))\n",
    "    print('Confidence:', res.get('confidence'))\n",
    "    print('Time:', res.get('time'))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8730dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_questions():\n",
    "    \"\"\"Load test questions for evaluation, using exact vanilla GPT-2 questions.\"\"\"\n",
    "    import pandas as pd\n",
    "    vanilla_df = pd.read_csv(\"../evaluation/vanilla_gpt2_results.csv\")\n",
    "    vanilla_questions = list(vanilla_df['Question'])\n",
    "    # Map from old question to ground truth/category if needed\n",
    "    question_map = {\n",
    "        \"What was Mahindra & Mahindra's total income from operations in 2023-24?\": {\n",
    "            \"ground_truth\": \"Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What was the PAT (Profit After Tax) for M&M standalone in 2023-24?\": {\n",
    "            \"ground_truth\": \"The PAT for M&M standalone in 2023-24 was ₹8,172 crores, representing a 64% increase compared to F23.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What was M&M's automotive volume in 2023-24?\": {\n",
    "            \"ground_truth\": \"Automotive volume for M&M in 2023-24 was 7,00,000 units.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What was the tractor volume for Mahindra in 2023-24?\": {\n",
    "            \"ground_truth\": \"Tractor volume for Mahindra in 2023-24 was 3,50,000 units.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What is Mahindra's market share in SUVs?\": {\n",
    "            \"ground_truth\": \"Mahindra's market share in SUVs is 20.4%.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What is Mahindra's market share in farm equipment?\": {\n",
    "            \"ground_truth\": \"Mahindra's market share in farm equipment is 41.7%.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What was the capex plan announced by Mahindra Group?\": {\n",
    "            \"ground_truth\": \"Mahindra Group announced an investment of INR 37,000 Crores across Auto, Farm and Services businesses (excluding Tech Mahindra) in F25, F26 and F27.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        },\n",
    "        \"What milestone did Mahindra Finance achieve in F24?\": {\n",
    "            \"ground_truth\": \"Mahindra Finance's loan book crossed the threshold of one lakh crores, increasing by 23% over the previous year.\",\n",
    "            \"category\": \"relevant_high_confidence\"\n",
    "        }\n",
    "        # Add more mappings as needed for all vanilla questions you want to evaluate\n",
    "    }\n",
    "    test_questions = []\n",
    "    for q in vanilla_questions:\n",
    "        meta = question_map.get(q, {\"ground_truth\": \"\", \"category\": \"\"})\n",
    "        test_questions.append({\n",
    "            \"question\": q,\n",
    "            \"ground_truth\": meta[\"ground_truth\"],\n",
    "            \"category\": meta[\"category\"]\n",
    "        })\n",
    "    return test_questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "90594cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model path: ../raft_finetuned_gpt2/final_model\n",
      "Files in model directory: ['config.json', 'generation_config.json', 'merges.txt', 'model.safetensors', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.json']\n",
      "Fine-tuned model and tokenizer loaded successfully.\n",
      "Fine-tuned model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 2. Fine-Tuned Model Loading (Robust)\n",
    "# =============================\n",
    "import os\n",
    "FT_MODEL_PATH = \"../raft_finetuned_gpt2/final_model\"  # Correct relative path from scripts folder\n",
    "fine_tuned_model = None\n",
    "fine_tuned_tokenizer = None\n",
    "fine_tuned_model_loaded = False\n",
    "print(f\"Checking model path: {FT_MODEL_PATH}\")\n",
    "if os.path.exists(FT_MODEL_PATH):\n",
    "    print(\"Files in model directory:\", os.listdir(FT_MODEL_PATH))\n",
    "    try:\n",
    "        fine_tuned_model = GPT2LMHeadModel.from_pretrained(FT_MODEL_PATH, local_files_only=True)\n",
    "        fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(FT_MODEL_PATH, local_files_only=True)\n",
    "        if fine_tuned_tokenizer.pad_token is None:\n",
    "            fine_tuned_tokenizer.add_special_tokens({'pad_token': fine_tuned_tokenizer.eos_token})\n",
    "        fine_tuned_model_loaded = True\n",
    "        print(\"Fine-tuned model and tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model or tokenizer: {e}\")\n",
    "        fine_tuned_model = None\n",
    "        fine_tuned_tokenizer = None\n",
    "        fine_tuned_model_loaded = False\n",
    "else:\n",
    "    print(\"Model path does not exist. Please check the path and try again.\")\n",
    "    fine_tuned_model_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0ad5588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 3. Metric Functions\n",
    "# =============================\n",
    "# Import necessary library for ROUGE\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    rouge_available = True\n",
    "except ImportError:\n",
    "    print(\"rouge_score library not found. Please install it (`pip install rouge_score`) to use ROUGE metric.\")\n",
    "    rouge_available = False\n",
    "\n",
    "def compute_confidence_score(question, answer, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Compute confidence score for a generated answer using average negative log-likelihood.\n",
    "    Confidence = exp(-loss), where loss is cross-entropy per token.\n",
    "    Based on the provided logic.\n",
    "    \"\"\"\n",
    "    # Encode full sequence (question + answer)\n",
    "    text = question + \" \" + answer\n",
    "    # Ensure the tokenizer has padding enabled and returns tensors\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move inputs to the same device as the model if necessary\n",
    "    if model.parameters():\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "    # Loss is average negative log likelihood\n",
    "    neg_log_likelihood = outputs.loss.item()\n",
    "\n",
    "    # Confidence = exp(-loss) → higher = more confident\n",
    "    confidence = torch.exp(-outputs.loss).item()\n",
    "\n",
    "    return confidence, neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0e2d7189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# 4. Evaluation Functions\n",
    "# =============================\n",
    "def evaluate_system(system_name, qa_function, test_questions):\n",
    "    \"\"\"Evaluate a QA system on test questions\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for q_data in test_questions:\n",
    "        query = q_data[\"question\"]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if system_name == \"RAG\":\n",
    "            # RAG system evaluation\n",
    "            is_valid, validation_msg = qa_function.input_validation(query)\n",
    "            if not is_valid:\n",
    "                answer = f\"Input validation failed: {validation_msg}\"\n",
    "                confidence = 0.0\n",
    "            else:\n",
    "                retrieved_docs = qa_function.hybrid_retrieval(query, k=3)\n",
    "                answer = qa_function.generate_response(query, retrieved_docs)\n",
    "                confidence = qa_function.get_confidence_score(query, retrieved_docs)\n",
    "        else:\n",
    "            # Fine-tuned system evaluation\n",
    "            answer, confidence, _ = qa_function(query)\n",
    "        \n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Simple correctness evaluation (would need manual review in practice)\n",
    "        correctness = \"Manual Review Required\"\n",
    "        if q_data[\"category\"] == \"irrelevant\" and \"validation failed\" in answer.lower():\n",
    "            correctness = \"Correct (Rejected irrelevant query)\"\n",
    "        elif q_data[\"category\"] == \"relevant_high_confidence\" and confidence > 50:\n",
    "            correctness = \"Likely Correct (High confidence)\"\n",
    "        elif q_data[\"category\"] == \"relevant_low_confidence\" and confidence < 50:\n",
    "            correctness = \"Expected Low Confidence\"\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": query,\n",
    "            \"ground_truth\": q_data[\"ground_truth\"],\n",
    "            \"category\": q_data[\"category\"],\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"response_time\": response_time,\n",
    "            \"correctness\": correctness\n",
    "        })\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5a1dcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(rag_results, ft_results):\n",
    "    \"\"\"Create a detailed comparison table\"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    for i, (rag_result, ft_result) in enumerate(zip(rag_results, ft_results)):\n",
    "        comparison_data.append({\n",
    "            \"Question_ID\": f\"Q{i+1}\",\n",
    "            \"Question\": rag_result[\"question\"][:50] + \"...\",\n",
    "            \"Category\": rag_result[\"category\"],\n",
    "            \"RAG_Answer\": rag_result[\"answer\"][:100] + \"...\",\n",
    "            \"RAG_Confidence\": f\"{rag_result['confidence']:.1f}%\",\n",
    "            \"RAG_Time\": f\"{rag_result['response_time']:.2f}s\",\n",
    "            \"RAG_Correctness\": rag_result[\"correctness\"],\n",
    "            \"FT_Answer\": ft_result[\"answer\"][:100] + \"...\",\n",
    "            \"FT_Confidence\": f\"{ft_result['confidence']:.1f}%\",\n",
    "            \"FT_Time\": f\"{ft_result['response_time']:.2f}s\",\n",
    "            \"FT_Correctness\": ft_result[\"correctness\"]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bb41b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(rag_results, ft_results):\n",
    "    \"\"\"Analyze and compare the results\"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    rag_avg_confidence = sum([r[\"confidence\"] for r in rag_results]) / len(rag_results)\n",
    "    ft_avg_confidence = sum([r[\"confidence\"] for r in ft_results]) / len(ft_results)\n",
    "    \n",
    "    rag_avg_time = sum([r[\"response_time\"] for r in rag_results]) / len(rag_results)\n",
    "    ft_avg_time = sum([r[\"response_time\"] for r in ft_results]) / len(ft_results)\n",
    "    \n",
    "    analysis[\"average_confidence\"] = {\n",
    "        \"RAG\": rag_avg_confidence,\n",
    "        \"Fine-Tuned\": ft_avg_confidence\n",
    "    }\n",
    "    \n",
    "    analysis[\"average_response_time\"] = {\n",
    "        \"RAG\": rag_avg_time,\n",
    "        \"Fine-Tuned\": ft_avg_time\n",
    "    }\n",
    "    \n",
    "    # Count correct responses by category\n",
    "    rag_correct_by_category = {}\n",
    "    ft_correct_by_category = {}\n",
    "    \n",
    "    for result in rag_results:\n",
    "        category = result[\"category\"]\n",
    "        if category not in rag_correct_by_category:\n",
    "            rag_correct_by_category[category] = 0\n",
    "        if \"Correct\" in result[\"correctness\"] or \"Likely Correct\" in result[\"correctness\"]:\n",
    "            rag_correct_by_category[category] += 1\n",
    "    \n",
    "    for result in ft_results:\n",
    "        category = result[\"category\"]\n",
    "        if category not in ft_correct_by_category:\n",
    "            ft_correct_by_category[category] = 0\n",
    "        if \"Correct\" in result[\"correctness\"] or \"Likely Correct\" in result[\"correctness\"]:\n",
    "            ft_correct_by_category[category] += 1\n",
    "    \n",
    "    analysis[\"correctness_by_category\"] = {\n",
    "        \"RAG\": rag_correct_by_category,\n",
    "        \"Fine-Tuned\": ft_correct_by_category\n",
    "    }\n",
    "    \n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0a1241a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5. Evaluation Execution\n",
    "# =============================\n",
    "def main():\n",
    "    print(\"Starting Testing and Evaluation...\")\n",
    "    # Dataset Preparation\n",
    "    test_questions = load_test_questions()\n",
    "    print(f\"Loaded {len(test_questions)} test questions.\")\n",
    "    # Model Initialization\n",
    "    # Use correct paths to the data folder\n",
    "    chunks_2022_23 = load_and_chunk_text(REPORT_2022_23, CHUNK_SIZES)\n",
    "    chunks_2023_24 = load_and_chunk_text(REPORT_2023_24, CHUNK_SIZES)\n",
    "    \n",
    "    all_chunks = {\n",
    "        size: chunks_2022_23[size] + chunks_2023_24[size]\n",
    "        for size in CHUNK_SIZES\n",
    "    }\n",
    "    \n",
    "    rag_system = RAGSystem()\n",
    "    rag_system.add_documents(all_chunks)\n",
    "    print(\"RAG system initialized.\")\n",
    "    \n",
    "    # Load the fine-tuned model and tokenizer\n",
    "    fine_tuned_model_path = \"/content/drive/My Drive/CAI/raft_finetuned_gpt2/final_model\"\n",
    "\n",
    "    try:\n",
    "        fine_tuned_model = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n",
    "        fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "        # Add padding token if necessary, consistent with training\n",
    "        if fine_tuned_tokenizer.pad_token is None:\n",
    "            fine_tuned_tokenizer.add_special_tokens({'pad_token': fine_tuned_tokenizer.eos_token})\n",
    "            fine_tuned_model.resize_token_embeddings(len(fine_tuned_tokenizer))\n",
    "\n",
    "\n",
    "        print(f\"Successfully loaded fine-tuned model from {fine_tuned_model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading fine-tuned model: {e}\")\n",
    "        fine_tuned_model = None\n",
    "        fine_tuned_tokenizer = None\n",
    "\n",
    "    # Ensure the retrieval system components are loaded\n",
    "    if 'index' not in locals() or index is None:\n",
    "        print(\"FAISS index not found. Attempting to load...\")\n",
    "        index_save_path = '/content/drive/My Drive/CAI/faiss_index.bin'\n",
    "        documents_save_path = '/content/drive/My Drive/CAI/document_data.pkl'\n",
    "        try:\n",
    "            index = faiss.read_index(index_save_path)\n",
    "            with open(documents_save_path, 'rb') as f:\n",
    "                document_data = pickle.load(f)\n",
    "            document_texts = document_data['texts']\n",
    "            document_filenames = document_data['filenames']\n",
    "            print(\"Successfully loaded FAISS index and document data for evaluation.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading FAISS index or document data for evaluation: {e}\")\n",
    "            index = None\n",
    "            document_texts = []\n",
    "\n",
    "    if 'retriever_model' not in locals() or retriever_model is None:\n",
    "         print(\"Retriever model not found. Initializing...\")\n",
    "         try:\n",
    "             retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "             print(\"Successfully initialized retriever model.\")\n",
    "         except Exception as e:\n",
    "             print(f\"Error initializing retriever model: {e}\")\n",
    "             retriever_model = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e3932731",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = load_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "83309927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RAG system...\n",
      "RAG evaluation complete.\n",
      "First RAG result: {'question': \"What was Mahindra & Mahindra's total income from operations in 2023-24?\", 'ground_truth': \"Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\", 'category': 'relevant_high_confidence', 'answer': 'A.', 'confidence': 71.75397491455078, 'response_time': 11.430427551269531, 'correctness': 'Manual Review Required'}\n",
      "RAG evaluation complete.\n",
      "First RAG result: {'question': \"What was Mahindra & Mahindra's total income from operations in 2023-24?\", 'ground_truth': \"Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\", 'category': 'relevant_high_confidence', 'answer': 'A.', 'confidence': 71.75397491455078, 'response_time': 11.430427551269531, 'correctness': 'Manual Review Required'}\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Evaluating RAG system...\")\n",
    "import re\n",
    "\n",
    "def clean_rag_answer(answer, query):\n",
    "    # Remove the question if echoed at the start\n",
    "    answer = answer.strip()\n",
    "    if answer.lower().startswith(query.lower()):\n",
    "        answer = answer[len(query):].strip()\n",
    "    # Remove 'Context:' and anything before it\n",
    "    if 'Context:' in answer:\n",
    "        answer = answer.split('Context:')[-1].strip()\n",
    "    # Remove any leading 'Q1:', 'A1:', etc.\n",
    "    answer = re.sub(r'^[QA]\\d*:\\s*', '', answer)\n",
    "    # Remove everything after 'Question:' if present\n",
    "    if 'Question:' in answer:\n",
    "        answer = answer.split('Question:')[0].strip()\n",
    "    # Optionally, keep only the first sentence (ends with . ! or ?)\n",
    "    match = re.match(r'(.+?[.!?])\\s', answer)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "    return answer.strip()\n",
    "\n",
    "if 'test_questions' not in locals():\n",
    "    test_questions = load_test_questions()\n",
    "if 'REPORT_2022_23' not in locals():\n",
    "    REPORT_2022_23 = \"../data/MM-Annual-Report-2022-23_cleaned.txt\"\n",
    "if 'REPORT_2023_24' not in locals():\n",
    "    REPORT_2023_24 = \"../data/MM-Annual-Report-2023-24_cleaned.txt\"\n",
    "if 'rag_system' not in locals():\n",
    "    # Re-initialize RAG system if not present\n",
    "    chunks_2022_23 = load_and_chunk_text(REPORT_2022_23, CHUNK_SIZES)\n",
    "    chunks_2023_24 = load_and_chunk_text(REPORT_2023_24, CHUNK_SIZES)\n",
    "    all_chunks = {size: chunks_2022_23[size] + chunks_2023_24[size] for size in CHUNK_SIZES}\n",
    "    rag_system = RAGSystem()\n",
    "    rag_system.add_documents(all_chunks)\n",
    "\n",
    "import time\n",
    "rag_results = []\n",
    "for q_data in test_questions:\n",
    "    query = q_data[\"question\"]\n",
    "    start_time = time.time()\n",
    "    # For all financial questions, skip input validation (assume all are financial)\n",
    "    # is_valid, validation_msg = rag_system.input_validation(query)\n",
    "    # if not is_valid:\n",
    "    #     answer = f\"Input validation failed: {validation_msg}\"\n",
    "    #     confidence = 0.0\n",
    "    # else:\n",
    "    retrieved_docs = rag_system.hybrid_retrieval(query, k=3)\n",
    "    answer = rag_system.generate_response(query, retrieved_docs)\n",
    "    answer = clean_rag_answer(answer, query)\n",
    "    confidence = rag_system.get_confidence_score(query, retrieved_docs)\n",
    "    response_time = time.time() - start_time\n",
    "    rag_results.append({\n",
    "        \"question\": query,\n",
    "        \"ground_truth\": q_data[\"ground_truth\"],\n",
    "        \"category\": q_data[\"category\"],\n",
    "        \"answer\": answer,\n",
    "        \"confidence\": confidence,\n",
    "        \"response_time\": response_time,\n",
    "        \"correctness\": \"Manual Review Required\"\n",
    "    })\n",
    "print(\"RAG evaluation complete.\")\n",
    "print(f\"First RAG result: {rag_results[0] if rag_results else 'No results'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3f33fd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Fine-tuned system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned GPT-2 evaluation complete.\n",
      "First FT result: {'question': \"What was Mahindra & Mahindra's total income from operations in 2023-24?\", 'ground_truth': \"Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\", 'category': 'relevant_high_confidence', 'answer': \"Mahindra & Mahindra's total income from operations in 2023-24 was ₹103,158 crores.\", 'confidence': 0.2663975656032562, 'response_time': 7.0835185050964355, 'correctness': 'Manual Review Required'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Evaluating Fine-tuned system...\")\n",
    "import re\n",
    "def clean_ft_answer(answer, query, qnum=None):\n",
    "    answer = answer.strip()\n",
    "    # If question numbering is present, extract only the answer for the current question\n",
    "    if qnum is not None:\n",
    "        # Look for 'A{qnum}:' and extract until next 'Q' or end\n",
    "        pattern = rf'A{qnum}:(.*?)(?:Q\\d+:|$)'\n",
    "        match = re.search(pattern, answer, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    # Fallback: Remove the question if echoed at the start\n",
    "    if answer.lower().startswith(query.lower()):\n",
    "        answer = answer[len(query):].strip()\n",
    "    # Remove 'Context:' and anything before it\n",
    "    if 'Context:' in answer:\n",
    "        answer = answer.split('Context:')[-1].strip()\n",
    "    # Remove any leading 'Q1:', 'A1:', etc.\n",
    "    answer = re.sub(r'^[QA]\\d*:\\s*', '', answer)\n",
    "    return answer.strip()\n",
    "\n",
    "def fine_tuned_qa(query, qnum=None):\n",
    "    if fine_tuned_model is None or fine_tuned_tokenizer is None:\n",
    "        return \"Model not loaded\", 0.0, None\n",
    "    # Encode input with attention mask for padding\n",
    "    inputs = fine_tuned_tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs.get(\"attention_mask\", None)\n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        output_ids = fine_tuned_model.generate(input_ids, attention_mask=attention_mask, max_length=128, num_return_sequences=1)\n",
    "        answer = fine_tuned_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        answer = clean_ft_answer(answer, query, qnum)\n",
    "        # Confidence score\n",
    "        confidence, neg_log_likelihood = compute_confidence_score(query, answer, fine_tuned_model, fine_tuned_tokenizer)\n",
    "    return answer, confidence, neg_log_likelihood\n",
    "\n",
    "try:\n",
    "    import time\n",
    "    ft_results = []\n",
    "    for idx, q_data in enumerate(test_questions):\n",
    "        query = q_data[\"question\"]\n",
    "        qnum = idx + 1  # Assume Q1, Q2, ...\n",
    "        start_time = time.time()\n",
    "        answer, confidence, _ = fine_tuned_qa(query, qnum)\n",
    "        response_time = time.time() - start_time\n",
    "        ft_results.append({\n",
    "            \"question\": query,\n",
    "            \"ground_truth\": q_data[\"ground_truth\"],\n",
    "            \"category\": q_data[\"category\"],\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"response_time\": response_time,\n",
    "            \"correctness\": \"Manual Review Required\"\n",
    "        })\n",
    "    print(\"Fine-Tuned GPT-2 evaluation complete.\")\n",
    "    print(f\"First FT result: {ft_results[0] if ft_results else 'No results'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Fine-Tuned GPT-2 evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "91af2cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis results saved to analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "# Analysis (fix numpy float32 serialization)\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(v) for v in obj]\n",
    "    elif hasattr(obj, 'item') and callable(obj.item):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "analysis = analyze_results(rag_results, ft_results)\n",
    "analysis_clean = convert_numpy(analysis)\n",
    "with open(\"analysis_results.json\", \"w\") as f:\n",
    "    json.dump(analysis_clean, f, indent=2)\n",
    "\n",
    "print(\"Analysis results saved to analysis_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "86f27c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION SUMMARY ===\n",
      "Average Confidence - RAG: 56.8%, Fine-Tuned: 0.5%\n",
      "Average Response Time - RAG: 7.29s, Fine-Tuned: 6.57s\n",
      "\n",
      "Comparison Table (first 5 rows):\n",
      "Question_ID                                              Question                 Category                                                                                                RAG_Answer RAG_Confidence RAG_Time                     RAG_Correctness                                                                                                FT_Answer FT_Confidence FT_Time          FT_Correctness\n",
      "         Q1            What was M&M's total income in 2023-24?... relevant_high_confidence M&M's net fair value gain on investments in equity shares at FVTOCI was $1,819,000,000.\\n\\nQuestion: W...          47.9%    8.00s              Manual Review Required  What was M&M's total income in 2023-24? Context: Q1: What was Mahindra & Mahindra's total income fro...          0.3%   5.13s  Manual Review Required\n",
      "         Q2 What is the company's expected growth rate for nex...  relevant_low_confidence                                                          Input validation failed: Query not financial....           0.0%    0.00s             Expected Low Confidence What is the company's expected growth rate for next year? Context: Mahindra &\\nMahindra un Ltd. MAHIN...          0.2%   3.32s Expected Low Confidence\n",
      "         Q3                     What is the capital of France?...               irrelevant                                                          Input validation failed: Query not financial....           0.0%    0.00s Correct (Rejected irrelevant query)  What is the capital of France? Context: Q1: What was Mahindra & Mahindra's total income from operati...          0.3%   3.22s  Manual Review Required\n",
      "         Q4    What was the PAT for M&M standalone in 2023-24?... relevant_high_confidence                                                          Input validation failed: Query not financial....           0.0%    0.00s              Manual Review Required  What was the PAT for M&M standalone in 2023-24? Context: Q1: What was Mahindra & Mahindra's total in...          0.3%   2.89s  Manual Review Required\n",
      "         Q5           What is Mahindra's market share in SUVs?... relevant_high_confidence  The market share in SUVs is a key indicator of the\\nindustry's growth prospects. The market share in ...          62.7%    6.47s    Likely Correct (High confidence)  What is Mahindra's market share in SUVs? Context: Q1: What was Mahindra & Mahindra's total income fr...          0.5%   3.98s  Manual Review Required\n",
      "\n",
      "Testing and Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "print(f\"Average Confidence - RAG: {analysis['average_confidence']['RAG']:.1f}%, Fine-Tuned: {analysis['average_confidence']['Fine-Tuned']:.1f}%\")\n",
    "print(f\"Average Response Time - RAG: {analysis['average_response_time']['RAG']:.2f}s, Fine-Tuned: {analysis['average_response_time']['Fine-Tuned']:.2f}s\")\n",
    "print(\"\\nComparison Table (first 5 rows):\")\n",
    "print(comparison_df.head().to_string(index=False))\n",
    "    \n",
    "print(\"\\nTesting and Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b19532a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 test questions.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare test questions\n",
    "test_questions = load_test_questions()\n",
    "print(f\"Loaded {len(test_questions)} test questions.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3e8d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (334984 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (359483 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (359483 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached RAG index from rag_index.pkl\n",
      "RAG system initialized.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Chunk annual reports and initialize RAG system\n",
    "chunks_2022_23 = load_and_chunk_text(REPORT_2022_23, CHUNK_SIZES)\n",
    "chunks_2023_24 = load_and_chunk_text(REPORT_2023_24, CHUNK_SIZES)\n",
    "all_chunks = {size: chunks_2022_23[size] + chunks_2023_24[size] for size in CHUNK_SIZES}\n",
    "rag_system = RAGSystem()\n",
    "rag_system.add_documents(all_chunks)\n",
    "print(\"RAG system initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "37b7f44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Evaluate RAG system\n",
    "rag_results = evaluate_system(\"RAG\", rag_system, test_questions)\n",
    "print(\"RAG evaluation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ed7e223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>confidence</th>\n",
       "      <th>inference_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'question': 'What was Mahindra &amp; Mahindra's t...</td>\n",
       "      <td>Error: 'RAGSystem' object has no attribute 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'question': 'What was the PAT (Profit After T...</td>\n",
       "      <td>Error: 'RAGSystem' object has no attribute 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'question': 'What was M&amp;M's automotive volume...</td>\n",
       "      <td>Error: 'RAGSystem' object has no attribute 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'question': 'What was the tractor volume for ...</td>\n",
       "      <td>Error: 'RAGSystem' object has no attribute 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'question': 'What is Mahindra's market share ...</td>\n",
       "      <td>Error: 'RAGSystem' object has no attribute 'an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  {'question': 'What was Mahindra & Mahindra's t...   \n",
       "1  {'question': 'What was the PAT (Profit After T...   \n",
       "2  {'question': 'What was M&M's automotive volume...   \n",
       "3  {'question': 'What was the tractor volume for ...   \n",
       "4  {'question': 'What is Mahindra's market share ...   \n",
       "\n",
       "                                              answer  confidence  \\\n",
       "0  Error: 'RAGSystem' object has no attribute 'an...           0   \n",
       "1  Error: 'RAGSystem' object has no attribute 'an...           0   \n",
       "2  Error: 'RAGSystem' object has no attribute 'an...           0   \n",
       "3  Error: 'RAGSystem' object has no attribute 'an...           0   \n",
       "4  Error: 'RAGSystem' object has no attribute 'an...           0   \n",
       "\n",
       "   inference_time  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG Evaluation (fix: prevent repeated answers)\n",
    "rag_results = []\n",
    "for idx, q in enumerate(test_questions):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        res = rag_system.answer_question(q)\n",
    "        answer = res[\"answer\"]\n",
    "        # Clean answer: remove repeated answers, extra context, and whitespace\n",
    "        # If answer contains multiple repeated blocks, keep only the first unique answer\n",
    "        if isinstance(answer, str):\n",
    "            # Split by newlines, remove duplicates, keep first non-empty unique line\n",
    "            lines = [l.strip() for l in answer.split(\"\\n\") if l.strip()]\n",
    "            seen = set()\n",
    "            cleaned_lines = []\n",
    "            for l in lines:\n",
    "                if l not in seen:\n",
    "                    cleaned_lines.append(l)\n",
    "                    seen.add(l)\n",
    "            # If the answer is repeated as a block, keep only the first block\n",
    "            if len(cleaned_lines) > 0:\n",
    "                answer = cleaned_lines[0]\n",
    "            else:\n",
    "                answer = \"\"\n",
    "        else:\n",
    "            answer = str(answer)\n",
    "        confidence = res.get(\"confidence\", 0)\n",
    "        response_time = time.time() - start_time\n",
    "        rag_results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"inference_time\": response_time\n",
    "        })\n",
    "    except Exception as e:\n",
    "        rag_results.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": f\"Error: {e}\",\n",
    "            \"confidence\": 0,\n",
    "            \"inference_time\": 0\n",
    "        })\n",
    "rag_df = pd.DataFrame(rag_results)\n",
    "rag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f2b5dfe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'response_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m---> 12\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrag_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m analysis_clean \u001b[38;5;241m=\u001b[39m convert_numpy(analysis)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[1;32mIn[167], line 9\u001b[0m, in \u001b[0;36manalyze_results\u001b[1;34m(rag_results, ft_results)\u001b[0m\n\u001b[0;32m      6\u001b[0m rag_avg_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rag_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rag_results)\n\u001b[0;32m      7\u001b[0m ft_avg_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ft_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ft_results)\n\u001b[1;32m----> 9\u001b[0m rag_avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rag_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rag_results)\n\u001b[0;32m     10\u001b[0m ft_avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ft_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ft_results)\n\u001b[0;32m     12\u001b[0m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_confidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_avg_confidence,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m: ft_avg_confidence\n\u001b[0;32m     15\u001b[0m }\n",
      "Cell \u001b[1;32mIn[167], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m rag_avg_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rag_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rag_results)\n\u001b[0;32m      7\u001b[0m ft_avg_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ft_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ft_results)\n\u001b[1;32m----> 9\u001b[0m rag_avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rag_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(rag_results)\n\u001b[0;32m     10\u001b[0m ft_avg_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m ft_results]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(ft_results)\n\u001b[0;32m     12\u001b[0m analysis[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_confidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG\u001b[39m\u001b[38;5;124m\"\u001b[39m: rag_avg_confidence,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-Tuned\u001b[39m\u001b[38;5;124m\"\u001b[39m: ft_avg_confidence\n\u001b[0;32m     15\u001b[0m }\n",
      "\u001b[1;31mKeyError\u001b[0m: 'response_time'"
     ]
    }
   ],
   "source": [
    "# Step 6: Analyze results and save analysis (fix numpy float32 serialization)\n",
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(v) for v in obj]\n",
    "    elif hasattr(obj, 'item') and callable(obj.item):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "analysis = analyze_results(rag_results, ft_results)\n",
    "analysis_clean = convert_numpy(analysis)\n",
    "with open(\"analysis_results.json\", \"w\") as f:\n",
    "    json.dump(analysis_clean, f, indent=2)\n",
    "print(\"Analysis results saved to analysis_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eff3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EVALUATION SUMMARY ===\n",
      "Average Confidence - RAG: 26.4%, Fine-Tuned: 0.4%\n",
      "Average Response Time - RAG: 3.36s, Fine-Tuned: 2.97s\n",
      "\n",
      "Comparison Table (first 5 rows):\n",
      "Question_ID                                              Question                 Category                                                                                                RAG_Answer RAG_Confidence RAG_Time                     RAG_Correctness                                                                                                FT_Answer FT_Confidence FT_Time          FT_Correctness\n",
      "         Q1            What was M&M's total income in 2023-24?... relevant_high_confidence M&M's net fair value gain on investments in equity shares at FVTOCI was $1,819,000,000.\\n\\nQuestion: W...          47.9%    8.00s              Manual Review Required  What was M&M's total income in 2023-24? Context: Q1: What was Mahindra & Mahindra's total income fro...          0.3%   5.13s  Manual Review Required\n",
      "         Q2 What is the company's expected growth rate for nex...  relevant_low_confidence                                                          Input validation failed: Query not financial....           0.0%    0.00s             Expected Low Confidence What is the company's expected growth rate for next year? Context: Mahindra &\\nMahindra un Ltd. MAHIN...          0.2%   3.32s Expected Low Confidence\n",
      "         Q3                     What is the capital of France?...               irrelevant                                                          Input validation failed: Query not financial....           0.0%    0.00s Correct (Rejected irrelevant query)  What is the capital of France? Context: Q1: What was Mahindra & Mahindra's total income from operati...          0.3%   3.22s  Manual Review Required\n",
      "         Q4    What was the PAT for M&M standalone in 2023-24?... relevant_high_confidence                                                          Input validation failed: Query not financial....           0.0%    0.00s              Manual Review Required  What was the PAT for M&M standalone in 2023-24? Context: Q1: What was Mahindra & Mahindra's total in...          0.3%   2.89s  Manual Review Required\n",
      "         Q5           What is Mahindra's market share in SUVs?... relevant_high_confidence  The market share in SUVs is a key indicator of the\\nindustry's growth prospects. The market share in ...          62.7%    6.47s    Likely Correct (High confidence)  What is Mahindra's market share in SUVs? Context: Q1: What was Mahindra & Mahindra's total income fr...          0.5%   3.98s  Manual Review Required\n",
      "\n",
      "Testing and Evaluation Complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Display summary and first 5 rows of comparison table\n",
    "print(\"\\n=== EVALUATION SUMMARY ===\")\n",
    "print(f\"Average Confidence - RAG: {analysis['average_confidence']['RAG']:.1f}%, Fine-Tuned: {analysis['average_confidence']['Fine-Tuned']:.1f}%\")\n",
    "print(f\"Average Response Time - RAG: {analysis['average_response_time']['RAG']:.2f}s, Fine-Tuned: {analysis['average_response_time']['Fine-Tuned']:.2f}s\")\n",
    "print(\"\\nComparison Table (first 5 rows):\")\n",
    "print(comparison_df.head().to_string(index=False))\n",
    "print(\"\\nTesting and Evaluation Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778955ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT markdown exported to ../evaluation/ft_results.md\n",
      "RAG markdown exported to ../evaluation/rag_results.md\n"
     ]
    }
   ],
   "source": [
    "# Export only FT and RAG results as markdown, matching vanilla GPT-2 format and questions\n",
    "import pandas as pd\n",
    "vanilla_df = pd.read_csv(\"../evaluation/vanilla_gpt2_results.csv\")\n",
    "vanilla_questions = list(vanilla_df['Question'])\n",
    "\n",
    "rag_df = pd.DataFrame(rag_results) if isinstance(rag_results, list) else rag_results\n",
    "ft_df = pd.DataFrame(ft_results) if isinstance(ft_results, list) else ft_results\n",
    "\n",
    "def get_result(df, question):\n",
    "    row = df[df['question'] == question]\n",
    "    if row.empty:\n",
    "        return {'answer': '', 'confidence': '', 'response_time': '', 'correctness': ''}\n",
    "    row = row.iloc[0]\n",
    "    return {\n",
    "        'answer': row.get('answer', ''),\n",
    "        'confidence': row.get('confidence', ''),\n",
    "        'response_time': row.get('response_time', ''),\n",
    "        'correctness': row.get('correctness', '')\n",
    "    }\n",
    "\n",
    "# FT markdown\n",
    "with open(\"../evaluation/ft_results.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# Fine-Tuned GPT-2 Model Results\\n\\n\")\n",
    "    for q in vanilla_questions:\n",
    "        res = get_result(ft_df, q)\n",
    "        f.write(f\"## Question: {q}\\n\")\n",
    "        f.write(f\"**Model:** Fine-Tuned GPT-2\\n\")\n",
    "        f.write(f\"**Model Answer:** {res['answer']}\\n\")\n",
    "        f.write(f\"**Confidence:** {res['confidence']}\\n\")\n",
    "        f.write(f\"**Time (s):** {res['response_time']}\\n\")\n",
    "        f.write(f\"**Correct (Y/N):** {res['correctness']}\\n\\n---\\n\\n\")\n",
    "print(\"FT markdown exported to ../evaluation/ft_results.md\")\n",
    "\n",
    "# RAG markdown\n",
    "with open(\"../evaluation/rag_results.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# RAG Model Results\\n\\n\")\n",
    "    for q in vanilla_questions:\n",
    "        res = get_result(rag_df, q)\n",
    "        f.write(f\"## Question: {q}\\n\")\n",
    "        f.write(f\"**Model:** RAG\\n\")\n",
    "        f.write(f\"**Model Answer:** {res['answer']}\\n\")\n",
    "        f.write(f\"**Confidence:** {res['confidence']}\\n\")\n",
    "        f.write(f\"**Time (s):** {res['response_time']}\\n\")\n",
    "        f.write(f\"**Correct (Y/N):** {res['correctness']}\\n\\n---\\n\\n\")\n",
    "print(\"RAG markdown exported to ../evaluation/rag_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01dc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
